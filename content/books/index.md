**Introduction to Large Language Models II: Implementation and Evaluation**

Publisher: Gijutsu-Hyohron(技術評論社), Sep 2024

*Chapter Author: Distributed Parallel Training*

Link: https://amzn.asia/d/aemtunI

Authored the chapter dedicated to distributed training infrastructure for Large Language Models. I focused on bridging the gap between theory and engineering, providing a deep dive into **Data Parallelism, DeepSpeed ZeRo, Pipeline Parallelism (PP), Tensor Parallelism (TP), and 3D Parallelism**.

* **Technical Contribution:** Designed and implemented hands-on tutorials for pre-training **Llama-2** from scratch, demonstrating complex parallelization strategies in code.
* **Impact:** The book is highly rated on Amazon Japan and serves as a key technical resource for LLM engineers in Japan.
