# Publications

## Peer-Reviewed Conference Papers

---

## Workshop Papers

### **Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese and Multilingual LLMs**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-First_Author-007ec6?style=flat-square" alt="First Author"></span>
  <a href="https://arxiv.org/abs/2412.14471"><img src="https://img.shields.io/badge/arXiv-2412.14471-b31b1b.svg?style=flat-square" alt="arXiv"></a>
  <a href="https://colmweb.org/"><img src="https://img.shields.io/badge/COLM_2025-MELT_Workshop-4b44ce?style=flat-square" alt="COLM 2025"></a>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Award-Outstanding_Paper-FFD700?style=flat-square" alt="Award"></span>
  <a href="YOUR_SLIDE_LINK_HERE"><img src="https://img.shields.io/badge/Slides-View_Deck-E4405F?logo=slideshare&style=flat-square" alt="Slides"></a>
</div>

* **Venue:** Conference on Language Modeling (COLM) Multilingual and Equitable Language Technologies Workshop 2025
* **Award:** Outstanding Paper Award, Natural Language Processing Research Meeting of the Information Processing Society of Japan (IPSJ-NLP)

### **llm-recipes: A Framework for Seamless Integration and Efficient Continual Pre-Training of Large Language Models**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://sc24.supercomputing.org/"><img src="https://img.shields.io/badge/SC_2024-TPC_Workshop-005696?style=flat-square" alt="SC 2024"></a>
  <a href="https://github.com/"><img src="https://img.shields.io/badge/GitHub-Repo-181717?logo=github&style=flat-square" alt="GitHub"></a>
  <a href="YOUR_SLIDE_LINK_HERE"><img src="https://img.shields.io/badge/Slides-View_Deck-E4405F?logo=slideshare&style=flat-square" alt="Slides"></a>
</div>

* **Venue:** The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC24), Trillion Parameter Consortium (TPC) Workshop

---

## Preprints

### **Rewriting Pre-Training Data Boosts LLM Performance in Math and Code**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://arxiv.org/abs/2505.02881"><img src="https://img.shields.io/badge/arXiv-2505.02881-b31b1b.svg?style=flat-square" alt="arXiv"></a>
  <a href="LINK"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-ffd21e?style=flat-square" alt="Hugging Face"></a>
  <a href="LINK"><img src="https://img.shields.io/badge/Code-GitHub-181717?logo=github&style=flat-square" alt="Code"></a>
</div>

* **Date:** 2025 (arXiv:2505.02881 [cs.LG])

### **Balancing Speed and Stability: The Trade-offs of FP8 vs. BF16 Training in LLMs**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://arxiv.org/abs/2411.08719"><img src="https://img.shields.io/badge/arXiv-2411.08719-b31b1b.svg?style=flat-square" alt="arXiv"></a>
</div>

* **Date:** 2024 (arXiv:2411.08719 [cs.LG])

### **LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://arxiv.org/abs/2407.03963"><img src="https://img.shields.io/badge/arXiv-2407.03963-b31b1b.svg?style=flat-square" alt="arXiv"></a>
  <a href="LINK"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-ffd21e?style=flat-square" alt="Hugging Face"></a>
</div>

* **Date:** 2024 (arXiv:2407.03963 [cs.CL])
* **Note:** Authors are listed in alphabetical order.

---

## Japanese Publications

### **継続事前学習による日本語に強い大規模言語モデルの構築**
(英訳: )

<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-First_Author-007ec6?style=flat-square" alt="First Author"></span>
  <a href="https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A8-5.pdf"><img src="https://img.shields.io/badge/NLP-2024-004513?style=flat-square" alt="NLP2024"></a>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Award-Excellent_Paper-FFD700?style=flat-square" alt="Award"></span>
  <a href="https://speakerdeck.com/fujiikazuki2000/yan-yu-chu-li-xue-hui-2024-ji-sok-shi-qian-xue-xi-niyoruri-ben-yu-niqiang-ida-gui-mo-yan-yu-moderunogou-zhu"><img src="https://img.shields.io/badge/Slides-View_Deck-E4405F?logo=slideshare&style=flat-square" alt="Slides"></a>
  <a href="https://huggingface.co/collections/tokyotech-llm/swallow"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-ffd21e?style=flat-square" alt="Hugging Face"></a>
  <a href="https://github.com/rioyokotalab/Megatron-Llama2"><img src="https://img.shields.io/badge/Code-GitHub-181717?logo=github&style=flat-square" alt="Code"></a>
</div>

* **Authors:** **藤井一喜(Kazuki Fujii)**, 中村泰士, Mengsay Loem, 飯田大貴, 大井聖也, 服部翔, 平井翔太, 水木栄, 横田理央, 岡崎直観
* **Venue:** 言語処理学会第30回年次大会 (NLP2024)
* **Award:** 優秀賞 (Selected as one of the best papers, 12/599)

### **Swallowコーパス: 日本語大規模ウェブコーパス**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/C3-3.pdf"><img src="https://img.shields.io/badge/NLP-2024-004513?style=flat-square" alt="NLP2024"></a>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Award-Excellent_Paper-FFD700?style=flat-square" alt="Award"></span>
</div>

* **Authors:** 岡崎直観, 服部翔, 平井翔太, 飯田大貴, 大井聖也, **藤井一喜(Kazuki Fujii)**, 中村泰士, Mengsay Loem, 横田理央, 水木栄
* **Venue:** 言語処理学会第30回年次大会 (NLP2024)
* **Award:** 優秀賞 (Selected as one of the best papers, 12/599)


### **大規模言語モデルの分散並列学習**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-First_Author-007ec6?style=flat-square" alt="First Author"></span>
  <a href="https://www.ipsj.or.jp/award/9faeag0000004emc-att/5J-02.pdf"><img src="https://img.shields.io/badge/IPSJ-86th-005696?style=flat-square" alt="IPSJ"></a>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Award-Best_Paper-FFD700?style=flat-square" alt="Award"></span>
  <a href="https://speakerdeck.com/fujiikazuki2000/qing-bao-chu-li-xue-hui-quan-guo-da-hui-2024-da-gui-mo-yan-yu-moderunofen-san-bing-lie-xue-xi"><img src="https://img.shields.io/badge/Slides-View_Deck-E4405F?logo=slideshare&style=flat-square" alt="Slides"></a>
  <a href="https://github.com/rioyokotalab/Megatron-Llama2"><img src="https://img.shields.io/badge/Code-GitHub-181717?logo=github&style=flat-square" alt="Code"></a>
</div>

* **Authors:** **藤井一喜(Kazuki Fujii)**, 横田理央
* **Venue:** 情報処理学会 第86回全国大会 (2024)
* **Award:** 大会優秀賞 (Best Paper Award of IPSJ National Convention)

### **Swallowコーパスv2: 教育的な日本語ウェブコーパスの構築**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/Q3-6.pdf"><img src="https://img.shields.io/badge/NLP-2025-004513?style=flat-square" alt="NLP2025"></a>
</div>

* **Authors:** 服部 翔, 岡崎 直観, 水木 栄, **藤井 一喜(Kazuki Fujii)**, 中村 泰士, 大井 聖也, (他)
* **Venue:** 言語処理学会第31回年次大会 (NLP2025)

### **模倣学習による大規模言語モデルの指示チューニング**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/Q8-21.pdf"><img src="https://img.shields.io/badge/NLP-2025-004513?style=flat-square" alt="NLP2025"></a>
</div>

* **Authors:** Youmi Ma, 水木 栄, **藤井 一喜(Kazuki Fujii)**, 中村 泰士, 大井 聖也, (他)
* **Venue:** 言語処理学会第31回年次大会 (NLP2025)

### **新聞記事からつくる 時事と社会に強い日本語LLM**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Co--Author-777777?style=flat-square" alt="Co-Author"></span>
  <a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C10-1.pdf"><img src="https://img.shields.io/badge/NLP-2025-004513?style=flat-square" alt="NLP2025"></a>
</div>

* **Authors:** 服部 翔, 水木 栄, **藤井 一喜(Kazuki Fujii)**, 中村 泰士, (他)
* **Venue:** 言語処理学会第31回年次大会 (NLP2025)

## Talks

### AWS Summit Japan 2025

**Title:** Amazon SageMaker HyperPod を利用した日本語 LLM (Swallow) の構築（CUS-02）
Youtube: https://www.youtube.com/watch?v=HnsnWjEQ6Jo&list=PLzWGOASvSx6GlBQPNhLRBDLqKgYHJHOml&index=108
Slides: https://speakerdeck.com/fujiikazuki2000/aws-summit-japan-2025-amazon-sagemaker-hyperpodwoli-yong-sitari-ben-yu-llm-swallow-nogou-zhu-cus-02
Japanese Talk



### Google Cloud Next '24 Tokyo


### DDN

### NLPコロキウム


### Tokyo Institute of Technology


### RIKEN AIP



### NLP2024 Invited Talk

https://sites.google.com/view/llm-discussion-nlp2024-ws

15:40 - 16:20 藤井一喜（東工大、Kotoba Technologies, Inc.）自然言語処理のための分散並列学習

https://speakerdeck.com/fujiikazuki2000/zi-ran-yan-yu-chu-li-notamenofen-san-bing-lie-xue-xi-3dd9cdf8-cc6d-4350-8141-89ce35b9d273

Japanese Talk

### SC Tokyo Tech Booth 2024

Continual Pre-Training on TSUBAME for a Target Language

Thursday, 21 November 2024
1:00 PM - 1:30 PM
Continual Pre-Training on TSUBAME for a Target Language.
Kazuki Fujii and Taishi Nakamura, Institute of Science Tokyo

This booth talk introduces a methodology for continual pre-training Llama-3 specializing in target languages while maintaining English proficiency. We present technical aspects of training configurations for 8B and 70B models, focusing on efficient adaptation strategies that preserve the model's original capabilities. We will also show practical considerations and technical tips for successfully training these large-scale models.

https://www.t4.cii.isct.ac.jp/sc24/booth_talks

English Talk

## Books

### **大規模言語モデル入門 Ⅱ 〜生成型LLMの実装と評価**
<div style="color: #666; font-size: 0.9em; margin-top: -15px; margin-bottom: 10px;">(Introduction to Large Language Models II: Implementation and Evaluation)</div>
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Role-Chapter_Author-007ec6?style=flat-square" alt="Role"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Publisher-Gijutsu--Hyohron-009944?style=flat-square" alt="Publisher"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Date-Sep_2024-777777?style=flat-square" alt="Date"></span>
  <a href="https://amzn.asia/d/aemtunI"><img src="https://img.shields.io/badge/Amazon-View_Book-FF9900?logo=amazon&style=flat-square" alt="Amazon"></a>
</div>

**Authored Chapter: Distributed Parallel Training**

I contributed as a technical author focusing on the scalability infrastructure for Large Language Models. I designed and implemented hands-on tutorials for **pre-training Llama-2 from scratch**, bridging the gap between theoretical concepts and production-grade engineering.

* **Core Topics:** Data Parallelism, DeepSpeed ZeRo, Pipeline Parallelism (PP), Tensor Parallelism (TP), and 3D Parallelism.
* **Impact:** Highly rated on Amazon Japan, serving as a definitive technical resource for LLM engineers in Japan.

## Technical Blogs

## Featured Articles (International)

### **Developing a 172B LLM with Strong Japanese Capabilities Using NVIDIA Megatron-LM**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <a href="https://developer.nvidia.com/blog/developing-a-172b-llm-with-strong-japanese-capabilities-using-nvidia-megatron-lm/"><img src="https://img.shields.io/badge/NVIDIA-Technical_Blog-76b900?logo=nvidia&style=flat-square" alt="NVIDIA Blog"></a>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Model-172B_Params-blue?style=flat-square" alt="172B"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tech-FP8_Training-orange?style=flat-square" alt="FP8"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Hardware-H100_Cluster-76b900?style=flat-square" alt="H100"></span>
</div>

**Summary:**
As part of the GENIAC initiative by Japan's METI, I led the training of a 172 billion parameter model from scratch. We utilized **NVIDIA H100 GPUs** and achieved a **1.4x speedup** (550 TFLOP/s) by using FP8 hybrid training with Megatron-Core and Transformer Engine.

* **Links:** [English Post](https://developer.nvidia.com/blog/developing-a-172b-llm-with-strong-japanese-capabilities-using-nvidia-megatron-lm/) | [Japanese Post](https://developer.nvidia.com/ja-jp/blog/developing-a-172b-llm-with-strong-japanese-capabilities-using-nvidia-megatron-lm/)

---

### **Training Llama 3.3 Swallow: A Japanese sovereign LLM on Amazon SageMaker HyperPod**
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <a href="https://aws.amazon.com/jp/blogs/machine-learning/training-llama-3-3-swallow-a-japanese-sovereign-llm-on-amazon-sagemaker-hyperpod/"><img src="https://img.shields.io/badge/AWS-Technical_Blog-232f3e?logo=amazon-aws&style=flat-square" alt="AWS Blog"></a>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Service-SageMaker_HyperPod-FF9900?style=flat-square" alt="SageMaker"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Model-Llama_3.3_70B-blue?style=flat-square" alt="Llama 3.3"></span>
</div>

**Summary:**
This technical report details the development of **Llama 3.3 Swallow (70B)**, which outperforms GPT-4o-mini in Japanese tasks. I discussed the infrastructure optimization on AWS SageMaker HyperPod and techniques for efficient continual pre-training.

* **Links:** [AWS Blog Post](https://aws.amazon.com/jp/blogs/machine-learning/training-llama-3-3-swallow-a-japanese-sovereign-llm-on-amazon-sagemaker-hyperpod/)

---

## Technical Articles (Japanese)

A collection of technical articles posted on **Zenn** (a popular engineering knowledge sharing platform in Japan).
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <a href="https://zenn.dev/kaz20"><img src="https://img.shields.io/badge/Zenn-My_Profile-3ea8ff?logo=zenn&style=flat-square" alt="Zenn Profile"></a>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Total_Likes-1400+-ff4b4b?style=flat-square" alt="Likes"></span>
</div>

### **LLM Development & Training Infrastructure**

**[大規模言語モデル(LLM)の作り方 Megatron-DeepSpeed編 Part1](https://zenn.dev/turing_motors/articles/04c1328bf6095a)**
<br>*(How to Build LLMs: Megatron-DeepSpeed Edition Part 1)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-186-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tech-Megatron--LM-76b900?style=flat-square" alt="Megatron"></span>
</div>

**[Swallow: LLaMA-2 日本語継続事前学習モデル](https://zenn.dev/tokyotech_lm/articles/d6cb3a8fdfc907)**
<br>*(Swallow: LLaMA-2 Continual Pre-training for Japanese)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-138-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Model-LLaMA--2-blue?style=flat-square" alt="LLaMA-2"></span>
</div>

**[大規模言語モデル(LLM)の作り方 GPT-NeoX編 Part 1](https://zenn.dev/turing_motors/articles/dff1466194f4ac)**
<br>*(How to Build LLMs: GPT-NeoX Edition Part 1)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-101-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tech-GPT--NeoX-green?style=flat-square" alt="NeoX"></span>
</div>

**[GENIAC: 172B 事前学習知見](https://zenn.dev/tokyotech_lm/articles/deb8012251bb68)**
<br>*(GENIAC: Insights from Pre-training a 172B Parameter Model)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-52-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Scale-172B-blue?style=flat-square" alt="172B"></span>
</div>

**[LLM開発の裏で行われるデバッグ作業: PyTorch DCP](https://zenn.dev/turing_motors/articles/04eed10b0aafe9)**
<br>*(Debugging Behind LLM Development: Deep Dive into PyTorch DCP)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-36-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tech-Debugging-yellow?style=flat-square" alt="Debug"></span>
</div>

### **Advanced Optimization & Techniques**

**[FP8 trainingを支える技術 1](https://zenn.dev/kaz20/articles/52fee6a3c9ea3b)**
<br>*(Technologies Supporting FP8 Training Part 1)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-42-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tech-FP8-orange?style=flat-square" alt="FP8"></span>
</div>

**[NVIDIA NeMoを利用したGPT-OSSの学習](https://zenn.dev/turing_motors/articles/81cf3128b22c63)**
<br>*(Training GPT-OSS using NVIDIA NeMo)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-62-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tech-NeMo-76b900?style=flat-square" alt="NeMo"></span>
</div>

**[Kotomamba: Mamba State Space Model 分散学習ライブラリ](https://zenn.dev/kotoba_tech/articles/3eb0984d8fdfb8)**
<br>*(Kotomamba: Distributed Training Library for Mamba SSM)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-44-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Arch-Mamba_SSM-purple?style=flat-square" alt="Mamba"></span>
</div>

### **Infrastructure & Tips**

**[Google Cloud: HPC Toolkitにて大規模深層学習環境を整備する](https://zenn.dev/tokyotech_lm/articles/6add0efaf07427)**
<br>*(Setting up Large-Scale Deep Learning Environments with Google Cloud HPC Toolkit)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Cloud-Google_Cloud-4285F4?style=flat-square" alt="GCP"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tool-HPC_Toolkit-blue?style=flat-square" alt="HPC"></span>
</div>

**[[Tips] PyTorchにおける動的リンク](https://zenn.dev/turing_motors/articles/3a434d046bbf48)**
<br>*(Dynamic Linking in PyTorch)*
<div style="display: flex; flex-wrap: wrap; gap: 5px; align-items: center; margin-top: -20px;">
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Likes-57-ff4b4b?style=flat-square" alt="Likes"></span>
  <span style="display: inline-block;"><img src="https://img.shields.io/badge/Tech-PyTorch-EE4C2C?style=flat-square" alt="PyTorch"></span>
</div>
